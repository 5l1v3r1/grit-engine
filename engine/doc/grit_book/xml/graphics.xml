<?xml version="1.0" encoding="UTF-8"?>

<section title="Graphics" id="graphics" splitbelow="true">

    The graphics subsystem is the largest, and its API gives control over
    everything you see on the screen.  This includes objects, particles, and
    lights in the graphics world.  It also includes the heads up display (HUD)
    and fonts.

    <section title="Graphics Bodies" id="gfx_body">

    </section>

    <section title="Materials and Shaders" id="gfx_materials_and_shaders">

        <def>Materials</def> and <def>shaders</def> define how each GfxBody in the world visually
        appears, e.g., their colour, how they react to light, etc.  Shaders run on the GPU and are
        written in a special high-level programming language called <def>Gasoline</def>.  The name
        Gasoline is derived from Grit Shading Language or GSL.  Materials are defined separately and
        are assigned to the triangles of your mesh.  They instantiate their shader's
        <def>parameters</def> with <def>texture</def> and <def>scalar</def> values to achieve the
        desired appearance.  They also have <def>system attributes</def> which control how Grit
        renders the material in a broader sense.

        <section title="Materials" id="gfx_materials">

            Materials have the following system attributes, shown below with their default values.

            <lua>
            material `Example` {

                -- What shader to use when rendering these triangles?
                shader = `/system/Default`,

                -- Either "OPAQUE", "ALPHA", or "ALPHA_DEPTH" (which also writes to depth buffer).
                sceneBlend = "OPAQUE",

                -- Whether to render both faces of the triangle or just the front.
                backfaces = false,

                -- Whether these triangles are rendered into the shadow map.
                castShadows = true,

                -- How much extra distance to move the shadow away from the caster
                -- (to avoid shadow acne).
                shadowBias = 0,

                -- Whether or not to check for discards from the dangs shader when drawing shadows.
                -- Note: This annotation will be inferred in a future version of Grit.
                shadowAlphaReject = true,

                -- Whether or not to use the additional lighting part of the shader.
                -- Note: This annotation will be inferred in a future version of Grit.
                additionalLighting = false,

                -- Number of bone blend weights at each vertex (for skeletal animation).
                -- Note: This annotation will be inferred in a future version of Grit.
                blendedBones = 0,
            }
            </lua>

            Further attributes in the material must match the parameters defined in the referenced
            shader.  One should find the definition of the shader to read what parameters are
            available.  See the following section for more information.

        </section>

        <section title="Shaders" id="gfx_shaders">

            A shader must be defined before it is used by a given material.  A shader definition
            contains three short Gasoline programs that run on the GPU -- <def>vertex</def>,
            <def>dangs</def>, and <def>additional</def>.  These respectively control how the
            vertexes are transformed to world space, how the inputs to the scene-wide lighting
            equation are calculated (diffuse, alpha, normal, gloss, specular), and what additional
            lighting if any is used, e.g., for effects like emissive lighting.  Note that in other
            engines, the vertex shader transforms to clip space via view and projection matrixes,
            but in Grit this latter part is handled automatically.  The user need only convert to
            world space.  Also in other engines, a single "fragment shader" is used to compute both
            the dangs, lighting equation, and additional lighting.  However in Grit, the lighting
            equation is controlled by the engine itself, and due to the deferred shading is a
            scene-wide lighting equation, and the additional lighting is separated so it can
            potentially be run in a subsequent additive pass.

            Gasoline is a high level language that makes it very easy to achieve special material
            effects on the GPU without having to know about the rendering techniques used by
            the engine itself.  The gasoline programs are automatically combined and augmented in
            various ways to produce real shaders for low level draw calls in either Direct3D
            or GL.  Internally, special shaders are generated for instanced geometry, shadow
            casting, wireframe, skeletal animation, deferred shading "forward" passes, deferred
            additional passes, alpha passes, first person rendering, skies, etc.  This is important,
            because the specific techniques needed for those kinds of rendering passes depend on the
            implementation of the engine itself and are subject to change in future versions while
            still supporting the same user-defined gasoline shaders.

            Gasoline has other features intended to make shader programming easier.  Type inference
            makes shaders more concise with less bureaucracy.  Automatic interpolator packing means
            that vertex attributes and variables defined in the vertex program are automatically
            available in the scope of the dangs and additional programs.  Using Gasoline is
            therefore similar to using the data-flow graphical shading languages (boxes and arrows)
            offered by other engines, except it is more powerful and better suited to version
            control tools.

            The shader is parameterised by textures and scalar values  The intention is that shaders
            can be generic enough to be used by a range materials, each achieving a different visual
            effect by binding different scalar values and textures to these parameters.  Some
            parameters are <def>static</def>.  This means a special low-level shader is generated in
            the implementation with the material value baked and optimized in.  This avoids paying
            the performance cost in in the case where e.g., a particular effect is not needed in a
            given material.

            The following example shows how to define shader parameters:

            <lua>
                shader `ExampleShader` {

                    -- A texture parameter:
                    -- Default: texture sampling will statically return Float4(r, g, b, a)
                    -- The rgba values usually range between 0 and 1 inclusive.
                    exampleTexture = uniform_texture_2d(r, g, b, a),

                    -- 3D textures represent depth with a stack of identically-sized 2d textures.
                    -- They are useful for geometry that is hard to wrap UVs around.
                    example3DTexture = uniform_texture_3d(r, g, b, a),

                    -- 1D textures are just a single line of texels.
                    example1DTexture = uniform_texture_1d(r, g, b, a),

                    -- Cube textures are the 6 faces of a cube.  They are useful for texture-mapping
                    -- convex sphere-like shapes.
                    example1DTexture = uniform_texture_cube(r, g, b, a),

                    -- Floating point scalar parameters of various dimensionality:
                    exampleFloat1 = uniform_float(0.5),
                    exampleFloat2 = uniform_float(1, 0.5),
                    exampleFloat3 = uniform_float(1, 1, 0.5),
                    exampleFloat4 = uniform_float(1, 2, 3, 0.5),

                    -- Static scalar parameters:
                    exampleStaticFloat1 = static_float(0.5),
                    exampleStaticFloat2 = static_float(1, 0.5),
                    exampleStaticFloat3 = static_float(1, 1, 0.5),
                    exampleStaticFloat4 = static_float(1, 2, 3, 0.5),

                    -- Integer scalar parameters:
                    exampleInt1 = uniform_int(4),
                    exampleInt2 = uniform_int(1, 7),
                    exampleInt3 = uniform_int(1, 1, 9),
                    exampleInt4 = uniform_int(1, 2, 3, 4),
                    exampleStaticInt1 = static_int(4),
                    exampleStaticInt2 = static_int(1, 7),
                    exampleStaticInt3 = static_int(1, 1, 9),
                    exampleStaticInt4 = static_int(1, 2, 3, 4),
                }
            </lua>


            A material can override the default value of a scalar parameter using a same-named
            attribute whose value is a number or appropriately-sized vector.  There are two ways a
            material can override textures:  Either referencing an image on disk, i.e. a <sref
            id="disk_resource_management">disk resource</sref>, or giving a solid colour.

            In the case of referencing an image, one can either give the image filename as a string,
            or when more power is needed, the various parameters of texture binding can be
            explicitly given in a table.

            To use a solid colour in place of a texture, a vector4 value can be given, which will
            appear as if a solid colour image was used.  As in the default texture colour, this
            colour is usually specified in the normalised range of 0 to 1 inclusive.

            <lua>
                material `ExampleMaterial` {

                    -- Explicit texture binding, various options can be overidden (defaults shown).
                    exampleTexture = {

                        -- Filename on disk.
                        image = `SomeFile.dds`,

                        -- "Maxification" filter.  Other options are "POINT" and "LINEAR".
                        filterMax = "ANISOTROPIC",

                        -- "Minification" filter.  Other options are "POINT" and "LINEAR".
                        filterMin = "ANISOTROPIC",

                        -- Mipmap filter.  Other options are "POINT" and "NONE".
                        filterMip = "LINEAR",

                        -- Anisotropy (only used if ANISOTROPIC filtering is used.
                        anisotropy = 16,

                        -- Addressing modes for the 3 texture coordinate components.
                        -- Other options are "CLAMP" "MIRROR" and "BORDER".
                        modeU = "WRAP",
                        modeV = "WRAP",
                        modeW = "WRAP",
                    },

                    -- Shorthand for the above, where all defaults are used.
                    exampleTexture = `SomeFile.dds`,

                    -- Solid colour (red, full alpha).
                    exampleTexture = vec(1, 0, 0, 1),

                    -- Override scalar parameters.
                    exampleFloat = 0.7,
                    exampleFloat2 = vec(0, 0),
                    exampleFloat3 = vec(1, 0, 1),
                    exampleFloat4 = vec(1, 2, 3, 4),
                    exampleStaticFloat = 0.7,
                    exampleStaticFloat2 = vec(0, 0),
                    exampleStaticFloat3 = vec(1, 0, 1),
                    exampleStaticFloat4 = vec(1, 2, 3, 4),
                }
            </lua>

            <section title="Gasoline Language" id="gfx_gasoline_language">

                Like all shading languages, Gasoline is a simple statically typed language that
                relies heavily on compiler optimisations to run efficiently on the GPU.

                Comments are ignored:

                <gasoline>
                    // This is a single line comment.

                    /* This is a multi-line comment.
                       It is terminated explicitly.
                     */ 
                </gasoline>

                Variable definitions:

                <gasoline>
                    // The type is inferred here (Float).
                    var my_variable = 8.0;

                    // An optional type annotation is checked and may make the code more readable.
                    var my_variable2 : Float = 8.0;

                    // If the type is given, the initializer can be omitted (it is zero).
                    var my_variable3 : Float;

                    // Numbers without decimal points are integers, so the inferred type is Int.
                    // If you wanted a Float, use 8.0 or type annotation.
                    var my_int = 8;

                    // Integers are automatically converted to floats if needed.
                    var my_float = 1 + 1.0;  // This is 2.0.

                    // Explicit conversion to a type is also possible.
                    var my_float2 = Float(1) + 1.0;
                </gasoline>

                A variety of types are supported:

                <gasoline>
                    var my_array = []Float { 1.0, 2.0, 3.0 };
                    var index = 1;
                    var tmp = my_array[i];
                    
                    // Vector types are available up to 4 elements (Int2-4 are similar).
                    var v2 = Float2(0, 10);
                    var v3 = Float3(v2, 1);
                    var v4 : Float4;  // All elements zero-initialized

                    // There members are accessible via "swizzling".
                    // The order is xyzw and rgba can also be used, mapping to the same values.
                    var example1 : Float = v2.y;  // This is 10.
                    var example2 : Float3 = v2.xxy;  // This is Float3(0, 0, 10).
                    var example3 : Float3 = v4.rgb;  // This is Float3(0, 0, 0).

                    // A value with a single dimension is automatically converted to fill all
                    // required channels.
                    var example4 : Float3 = 1.0;  // This is Float3(1, 1, 1).

                    // There are also booleans (true or false):
                    var b = true;
                    var b2 : Bool;  // Initialized to false.
                </gasoline>

                Variables are mutable (can have their values updated):

                <gasoline>
                    index = 2;
                    my_array[index] = 3;  // 3, an Int, is automatically converted to Float.
                    example2.r = 3;
                    example2.yz = Float2(1, 1);
                </gasoline>

                Functions can be called, using a C-like syntax:

                <gasoline>
                    out.normal = normalise(v);
                </gasoline>

                Listed below are all the functions with their type signatures (the parameters they
                        must be given and what they return).  The short-hand notation
                <code>Floatn</code> and <code>Intn</code> is not valid Gasoline but we use it here
                for conciseness.  It means that a version of the function is available for any size
                <code>n</code>.  Most mathematical functions are lifted up to vector types by
                performing the same operation on every element (pointwise).

                <gasoline>
                // Trigonometry
                tan: (Floatn) -> Floatn
                atan: (Floatn) -> Floatn
                sin: (Floatn) -> Floatn
                asin: (Floatn) -> Floatn
                cos: (Floatn) -> Floatn
                acos: (Floatn) -> Floatn
                atan2: (Float, Float) -> Float

                // Other simple math
                abs: (Floatn) -> Floatn
                abs: (Intn) -> Intn
                fract: (Floatn) -> Floatn
                floor: (Floatn) -> Floatn
                ceil: (Floatn) -> Floatn
                sqrt: (Floatn) -> Floatn
                pow: (Floatn, Float) -> Floatn
                strength: (Float, Float) -> Float  // Like pow() but clamps first param > 0

                // Compare value to neighbouring fragment.
                ddx: (Floatn) -> Floatn  // Fragment shader only
                ddy: (Floatn) -> Floatn  // Fragment shader only

                // Vectors
                dot: (Float2, Float2) -> Float
                dot: (Float3, Float3) -> Float
                normalise: (Float3) -> Float3
                reflect: (Float3, Float3, Float3) -> Float3
                cross: (Float3, Float3) -> Float3
                rotate_to_world: (Float3) -> Float3  // Vertex shader only
                transform_to_world: (Float3) -> Float3  // Vertex shader only
                length: (Floatn) -> Float
                
                // Matrix multiplication
                mul: (Matrix2x2, Float2) -> Float2
                mul: (Matrix2x3, Float2) -> Float3
                mul: (Matrix2x4, Float2) -> Float4
                mul: (Matrix3x2, Float3) -> Float2
                mul: (Matrix3x3, Float3) -> Float3
                mul: (Matrix3x4, Float3) -> Float4
                mul: (Matrix4x2, Float4) -> Float2
                mul: (Matrix4x3, Float4) -> Float3
                mul: (Matrix4x4, Float4) -> Float4

                // Colours
                desaturate: (Float3, Float) -> Float3
                pma_decode: (Float4) -> Float4  // Pre-multiplied alpha
                gamma_decode: (Floatn) -> Floatn
                gamma_encode: (Floatn) -> Floatn

                // Limits
                clamp: (Floatn, Floatn, Floatn) -> Floatn
                lerp: (Floatn, Floatn, Floatn) -> Floatn
                max: (Floatn, Floatn) -> Floatn
                min: (Floatn, Floatn) -> Floatn

                // Texture sampling
                // sample(t, coord) == sampleGrad(t, coord, ddx(coord), ddy(coord))
                sample: (FloatTexture1D, Float) -> Float4
                sample: (FloatTexture2D, Float2) -> Float4
                sample: (FloatTexture3D, Float3) -> Float4
                sample: (FloatTextureCube, Float3) -> Float4
                // Explicitly specify x and y differentials.
                sampleGrad: (FloatTexture1D, Float, Float, Float) -> Float4
                sampleGrad: (FloatTexture2D, Float2, Float2, Float2) -> Float4
                sampleGrad: (FloatTexture3D, Float3, Float3, Float3) -> Float4
                sampleGrad: (FloatTextureCube, Float3, Float3, Float3) -> Float4
                // Explicitly select a mipmap level.
                sampleLod: (FloatTexture1D, Float, Float) -> Float4
                sampleLod: (FloatTexture2D, Float2, Float) -> Float4
                sampleLod: (FloatTexture3D, Float3, Float) -> Float4
                sampleLod: (FloatTextureCube, Float3, Float) -> Float4
                </gasoline>

                The following arithmetic operators can be used, ordered by precedence.  All are
                left-associative, which means <code>A + B + C</code> is parsed as <code>(A + B) +
                C</code>.  Aside from <code>!</code>, <code>&amp;&amp;</code>, and <code>||</code>,
                which apply only to booleans, the rest apply to <code>Float</code>,
                <code>Float2</code>, <code>Float3</code>, <code>Float4</code>, <code>Int</code>,
                <code>Int2</code>, <code>Int3</code>, and <code>Int4</code>.  If a binary operator
                is applied to two different types, either one of them is automatically converted to
                the other.

                For example: <code>3 + 5.0</code> becomes <code>3.0 + 5.0</code>, and <code>3 +
                Int2(1, 0) </code>becomes <code>Int2(3, 3) + Int2(1, 0)</code>.  However,
                <code>Float2(1, 1) + Float3(1, 1, 1)</code> is an error since neither can be
                converted to the other.

                <pre>
                    Binding order:
                    f() f.g arr[e]  (function application, field and array indexing)
                    ! -  (unary negation)
                    * / %
                    + -
                    &lt; &lt;= &gt; &gt;=
                    == !=  (equal, not equal)
                    &amp;&amp; ||  ('and' and 'or')
                </pre>

                The shader parameters are available through the <code>mat</code> object.  Accessing
                uniform_float or uniform_int simply gets you a value of the appropriate type.
                Accessing textures gets you a texture object of the appropriate type, which can be
                used only as the first parameter of one of the <code>sample*</code> functions listed
                above.  It is an error to access a field of mat that was not declared as a shader
                parameter of some form.

                <gasoline>
                    out.colour = mat.myParameter * sample(mat.myTexture, uv);
                </gasoline>

                Scene-wide values are available through the <code>global</code> object.

                <gasoline>
                global.cameraPos : Float3  // World space position of the camera.
                global.fovY : Float  // Field of view (degrees)
                global.time : Float  // (see below)
                global.viewportSize : Float2  // Resolution of screen

                // The visible depth range.
                global.nearClipDistance : Float
                global.farClipDistance : Float

                // Lighting parameters.
                global.particleAmbient : Float3
                global.sunlightDiffuse : Float3
                global.sunlightDirection : Float3
                global.sunlightSpecular : Float3

                // Fog parameters.
                global.fogColour : Float3
                global.fogDensity : Float

                // Sky parameters.
                global.hellColour : Float3
                global.skyCloudColour : Float3
                global.skyCloudCoverage : Float
                global.skyGlareHorizonElevation : Float
                global.skyGlareSunDistance : Float
                global.sunAlpha : Float
                global.sunColour : Float3
                global.sunDirection : Float3
                global.sunFalloffDistance : Float
                global.sunSize : Float
                global.skyDivider1 : Float
                global.skyDivider2 : Float
                global.skyDivider3 : Float
                global.skyDivider4 : Float
                global.skyColour0 : Float3
                global.skyColour1 : Float3
                global.skyColour2 : Float3
                global.skyColour3 : Float3
                global.skyColour4 : Float3
                global.skyColour5 : Float3
                global.skySunColour0 : Float3
                global.skySunColour1 : Float3
                global.skySunColour2 : Float3
                global.skySunColour3 : Float3
                global.skySunColour4 : Float3
                </gasoline>

                There are also properties associated with individual GfxBody objects.  Currently,
                the following attributes are available for general use.  They are set using
                <code>mybody:setPaintColour(i, vec(r, g, b), m, s, g)</code> for i in 0 to 3.  In
                future, you will be able to define your own and call them whatever you want.

                <gasoline>
                body.paintDiffuse0 : Float3
                body.paintDiffuse1 : Float3
                body.paintDiffuse2 : Float3
                body.paintDiffuse3 : Float3
                body.paintMetallic0 : Float1
                body.paintMetallic1 : Float1
                body.paintMetallic2 : Float1
                body.paintMetallic3 : Float1
                body.paintSpecular0 : Float1
                body.paintSpecular1 : Float1
                body.paintSpecular2 : Float1
                body.paintSpecular3 : Float1
                body.paintGloss0 : Float1
                body.paintGloss1 : Float1
                body.paintGloss2 : Float1
                body.paintGloss3 : Float1
                </gasoline>

                Gasoline also has control flow constructs, i.e. <def>conditionals</def>,
                <def>discard</def>, and <def>loops</def>.  Since these constructs cause the
                execution to diverge between neighbouring pixels, <code>ddx</code> /
                <code>ddy</code> may not be used.  This also includes use of
                <code>sample(...)</code> since that internally uses <code>ddx</code> and
                <code>ddy</code>.

                <gasoline>
                if (expr) {
                    // This part only executed if expr evaluated to true.
                } else {
                    // This part only if it was false.
                }

                // The else branch is also optional:
                if (expr) {
                    // This part only executed if expr evaluated to true.
                }

                // The discard statement causes the pixel to not be drawn.
                // Typically it is used in a conditional, otherwise no pixels would be drawn.
                if (alpha &lt; 0.5) {
                    // Typically discard is used for alpha rejection effects.
                    discard;
                }
                // Note that ddx / ddy / sample() cannot be used after a possible discard.

                // For loops are similar to C:
                for (var x = 0; x &lt; 10; x = x + 1) {
                    // This part executed 10 times.
                }
                </gasoline>

            </section>

            <section title="Gasoline Vertex Shader" id="gfx_gasoline_vertex">

                The <def>vertex shader</def> is run once for each vertex in the mesh.  It takes the
                attributes at each vertex: position, normal, tangent, colours, texture coordinates,
                and computes the following: 1) The world space position of this vertex within the
                scene.  2) Any intermediate values that are used by the DANGS and additional
                shaders, but are more efficiently computed in the vertex shader and linearly
                interpolated across the triangle face.

                The <code>vert</code> object provides access to the various values from the mesh,
                each of which is a 4-dimensional vector.  The texture coordinates need not be used
                for sampling textures.

                <gasoline>
                    vert.position  -- object space
                    vert.colour 
                    vert.normal
                    vert.tangent
                    vert.coord0
                    vert.coord1
                    vert.coord2
                    vert.coord3
                    vert.coord4
                    vert.coord5
                    vert.coord6
                    vert.coord7
                </gasoline>

                The shader may write the world position of the vertex in <code>out.position</code>.
                The function transform_to_world is available to do this, but the shader can modify
                the position either before or after this transformation (or both) in order to
                achieve special effects such as breathing, swaying, etc.  The following default
                value is otherwise used:

                <gasoline>
                    // Default value:
                    out.position = transform_to_world(vert.position);
                </gasoline>

            </section>

            <section title="Gasoline DANGS Shader" id="gfx_gasoline_dangs">

                The <def>DANGS</def> shader computes the <def>diffuse</def>, <def>alpha</def>,
                <def>normal</def>, <def>gloss</def>, and <def>specular</def> components of the
                lighting equation for each pixel a triangle covers on the screen.

                The following per-pixel values are available:

                <gasoline>
                    // The screen coordinate
                    // Ranges from Float2(0, 0) at bottom left to global.viewportSize.
                    frag.screen
                </gasoline>

                The vert object is also available, the vertex attributes from the mesh are
                interpolated across the triangle.  Likewise, all variables defined in the vertex
                shader are also available and are interpolated.

                The shader may write the following lighting equation parameters into the
                <code>out</code> object.  Otherwise, the following defaults are used:

                <gasoline>
                    // Proportion of light absorbed and re-emitted in an arbitrary
                    // direction due to being absorbed into the molecules of the surface.  Since
                    // this process often changes the colour of the light, this input to the
                    // lighting equation is broken down by channel.
                    out.diffuse = Float3(0.5, 0.25, 0);

                    // 0 is totally transparent, 1 is totally opaque.  This has no effect if the
                    // material's sceneBlend attribute is OPAQUE.
                    out.alpha = 1;

                    // The world-space vector perpendicular to the surface at this point.
                    // This vector will be normalised by the engine.
                    out.normal = Float3(0, 0, 1);

                    // How polished is the surface (0 to 1).
                    out.gloss = 1;

                    // Proportion of the light that is reflected off the surface of the material
                    // like a mirror.  This is usually high in metals and low in organic materials.
                    out.specular = 0.04;
                </gasoline>

                Note that all these outputs are in <def>linear</def> space ready to be immediately
                processed by the lighting equation.  Most textures have <def>gamma encoded</def> red
                green and blue channels in order to concentrate more fidelity into the darker shades
                where the human eye is most sensitive.  This is true of textures edited by hand in
                image processing software and photo-sourced images.  Such textures must be
                gamma-decoded before being used, with the gamma_decode(colour) function.

                Note also that if a texture has <def>pre-multiplied alpha</def>, its red green and
                blue channels must be divided by the alpha channel before use (but after gamma
                decoding).

            </section>

            <section title="Gasoline Additional Shader" id="gfx_gasoline_additional">

                The <def>additional shader</def> allows a surface to emit light for a reason other
                than the scene-wide physically-based lighting equation.  For example if the surface
                is hot and radiating light, or due to bioluminescence, flourescence in UV light,
                LEDs or supernatural effects.  The colour output is in linear space:  0 to 1 in
                standard range, higher for HDR.  The <code>additionalLighting</code> material
                attribute must be set if this shader returns anything other than Float3(0, 0, 0).

                The vert object and vertex shader variables are available, as in the DANGS shader.

                <gasoline>
                    -- Additional light originating from this point and entering the camera.
                    out.colour = Float3(0, 0, 0);

                    -- 0 is totally transparent, 1 is totally opaque.
                    -- When rendering skies, this is used for blending purposes.
                    out.alpha = 1;
                </gasoline>

            </section>

        </section>

        <section title="Complete Example" id="gfx_materials_example">

            The following example is a simple shader and material that has a diffuse map and can
            also emit "emissive" light, i.e. light that glows from the surface itself independently
            of other light sources in the scene.

            <lua>
                shader `ExampleShader` {

                    -- If a texture is not provided in the material, texture samples will return
                    -- grey, i.e., Float4(0.5, 0.5, 0.5, 1).
                    diffuseMap = uniform_texture_2d(0.5, 0.5, 0.5, 1),

                    -- Useful for tinting the texture for variety.  The default of 1 has no effect.
                    -- Note that this parameter requires a 3d vector.
                    diffuseMask = uniform_float(1, 1, 1),

                    -- If alpha &lt;= this then discard entire fragment.
                    alphaRejectThreshold = uniform_float(-1),

                    -- Note that the default colour's alpha channel can be omitted, in which case
                    -- it defaults to 1.
                    -- Note that emissiveMask defaults to zero so must be overridden or emissiveMap
                    -- has no effect.
                    emissiveMap = uniform_texture_2d(1, 1, 1),

                    -- Note that this parameter can have values > 1 to power up an emissive texture
                    -- to HDR levels.
                    emissiveMask = uniform_float(0, 0, 0);

                    -- How the vertexes are transformed from object space to world space.
                    vertexCode = [[
                        out.position = transform_to_world(vert.position.xyz);
                        var normal_ws = rotate_to_world(vert.normal.xyz);
                    ]],

                    -- How the inputs to the lighting equation are computed.
                    -- Diffuse Alpha Normal Gloss Specular (DANGS)
                    dangsCode = [[
                        var diff_texel = sample(mat.diffuseMap, vert.coord0.xy);
                        out.diffuse = gamma_decode(diff_texel.rgb) * mat.diffuseMask;
                        out.alpha = diff_texel.a;
                        if (out.alpha &lt;= mat.alphaRejectThreshold) discard;
                        out.normal = normal_ws;
                        out.gloss = 0;
                        out.specular = 0.04;
                    ]],

                    -- Any additional light not due to the scene-wide lighting equation.
                    additionalCode = [[
                        var c = sample(mat.emissiveMap, vert.coord0.xy);
                        out.colour = gamma_decode(c.rgb) * mat.emissiveMask;
                    ]],
                }

                material `Example` {
                    // System attributes
                    shader = `ExampleShader`,
                    backfaces = true,

                    // Shader attributes
                    diffuseMap = `MyTexture.dds`,
                    //
                    diffuseMask = vec(0.8, 0.8, 0.5),
                }
            </lua>

        </section>


    </section>


    <section title="Realtime Shadow Techniques Used By Grit" id="gfx_shadow">

        <section title="Depth Shadow Mapping" id="gfx_shadow_dsm">

            Grit has fully dynamic shadows that are calculated in real time on
            the GPU. The basic technique is called <web
            url="http://en.wikipedia.org/wiki/Shadow_mapping">depth shadow
            mapping</web>.  It involves rendering the scene from the light (the
            sun or the moon) into a texture, called a <def>depth shadow
            map</def>. The shadow map is updated every frame, because objects
            move and so does the light.  The colour of the scene is not
            computed, as we are only interested in the distance to all the
            surfaces that the light can 'see' (these are the occluders).

            When the scene is rendered from the player's point of view, this
            shadow map is used as a reference to help decide if a given pixel
            is the closest one to the light (in which case it is not in shadow)
            or whether there is something else that is closer (in which case it
            is rendered darker because it is in shadow).

        </section>

        <section title="Perspective Transform" id="gfx_shadow_lispsm">

            The depth shadow map has a limited resolution, so in order to
            increase the apparent fidelity (and avoid blocky artefacts) there
            is a perspective transform applied in order to concentrate as many
            as possible of the shadow map's texels close to the player.  There
            are many techniques but the one used in Grit is called LiSPSM (<web
            url="http://www.cg.tuwien.ac.at/research/vr/lispsm/">LIght Space
            Perspective Shadow Mapping</web>).  The worst case is when the sun
            is directly behind you, in which case no perspective transform can
            be applied, and the shadow is very low detail and noisy.  However,
            if you look 90 degrees to the sun, the shadows will be a lot
            crisper due to the use of LiSPSM.  Note that increasing the
            resolution of the shadow map texture will also make the shadows
            crisper, but will cost memory and performance.

            The perspective transform changes every frame depending on the
            light direction and the chase cam's direction. Sometimes the
            changes can be quite severe. This causes an unavoidable 'crawling'
            effect in the shadows.

        </section>

        <section title="Covering Larger Distances" id="gfx_shadow_pssm">

            There are in fact 3 shadow maps used. One for the area closest to
            the player, one to cover the area further away, and the 3rd one for
            the furthest reach of the shadow (200 metres). They are all the
            same size textures, but the one nearest to the camera covers a much
            smaller area and thus the shadows are better defined. Another way
            of looking at this is that it allows shadows to appear much further
            from the player, without compromising the quality of shadows near
            the player. The exact technique used in Grit is called PSSM (<web
            url="http://http.developer.nvidia.com/GPUGems3/gpugems3_ch10.html">Parallel
            Split Shadow Mapping</web>). Sometimes you can see the transition
            from one shadow map to the next, as a sudden decrease in shadow
            quality.

        </section>

        <section title="Soft Shadows" id="gfx_shadow_pcf">

            If each screen pixel was merely tested for being in shadow or not,
            the shadows would be very hard-edged because of the sudden
            transition from 'in shadow' to 'not in shadow'. To avoid this, we
            soften the shadows using a technique called PCF (<web
            url="http://http.developer.nvidia.com/GPUGems/gpugems_ch11.html">Percentage
            Closer Filtering</web>) . This boils down to testing the shadow map
            several times per screen pixel, and taking the average.  The
            appearance is that several faint shadows have been overlaid in
            slightly different positions, to produce a blurred effect. It can
            get very slow but there is hardware support that we are currently
            not using that can help, see <issue id="125"/>.

        </section>

    </section>

    <section title="Shadow Artefacts" id="gfx_shadow_artefacts">

        There are certain things that can go wrong with dynamic shadow
        implementations like the ones used in Grit. There are some things to
        avoid when modelling objects, in order to avoid problems.

        <section title="Holes in shadows" id="gfx_shadow_holes">

            Since the shadows are calculated by rendering the scene from the
            sun (or moon) you have to make sure that your geometry, when viewed
            from this direction, appears to be opaque. This means cliffs must
            have polygons around the back facing the sun, in order to the sun
            shining through them to the front.  A more expensive alternative is
            to turn on the rendering of backfaces in the material.

            If your map is an island that drops below sealevel in all
            directions, you don't have to worry about this.  But if your map is
            surrounded by some sort of "wall", then you do.

        </section>

        <section title="Shadow Texture Stretch" id="gfx_shadow_stretch">

            <image src="shadow_stretch.jpg" title="Using SHADOWYNESS">
                Shadow texture stretch occurs where polygons do not face the
                light.
            </image>

            Since the shadow texture is projected onto the scene from the
            light, surfaces that are perpendicular to the light (e.g. flat
            ground at sunset) will experience very bad texture stretch. This
            causes aliasing artefacts. Because of the LiSPSM perspective
            transformation, the artefacts have a very nasty sawtooth
            appearance, instead of the square pixelation that usually occurs
            with aliasing artefacts.

            To visualise the aliasing, we can use the following, which renders
            just the projection of the shadow map onto the scene, with equal
            intensitity for all triangles:

            <lua>
                debug_cfg.falseColour = "SHADOWYNESS"
            </lua>

            A small fern on the edge of a cliff is projecting a shadow downhill
            away from the edge of the cliff.  The shadow is very elongated
            because of the low sun.  One can see the sawtooth artefacts in the
            stretched part of the shadow.  When animated, the moving sun causes
            these sawtooth artefacts to crawl in a very distracting way.

            <image src="shadow_stretched_hidden.jpg" title="Using SHADOW_MASK">
            Shadow texture stretch is usually hidden by the lighting equation.
            </image>

            Luckily these areas should receive very much light due to the
            diffuse lighting equation. E.g. if the light is incident at 15
            degrees then the amount of lighting would only be 25% (i.e.
            sin(15)) of the amount of light that it would receive at 90
            degrees. This means the shadow is much less distinct in these
            areas. The following falseColour value shows the actual shadow,
            i.e.  incorporating the diffuse lighting component as well as the
            shadow map:

            <lua>
                debug_cfg.falseColour = "SHADOW_MASK"
            </lua>

            In the next section, we can see how this effect can be disrupted by
            certain kinds of assets.

        </section>

        <section title="Normal Bending" id="gfx_shadow_bending">

            If your mesh has sharp edges between polys (an angle of more than
            20 degrees for example) and is smooth shaded, then for some pixels,
            the normals interpolated across that mesh will be considerably
            different to the 'true' normal of that face (i.e. the normal you
            would calculate using the positions of the 3 vertexes).  For
            example, if you model a cube and use smooth shading then the
            normals of each face will be orthogonal, but the normals will be
            smoothly interpolated around the cube causing a huge amount of
            normal bending at the edges and corners.

            <image src="shadow_artefacts.jpg" title="Normal Bending">
            Shadow artefacts caused by normal bending.
            </image>

            Normal bending is usually OK, but causes a problem with shadows.
            This is because shadow stretch occurs in places where the true
            normal of the polygon is close to perpendicular to the light
            source, however light attenuation uses the interpolated normal,
            which can be pointing closer to the light than the true normal.
            This kind of artefact often occurs on sharp terrain like cliffs.
            It causes areas to be illuminated when they would not otherwise be,
            and therefore causes shadow artefacts to appear that would
            ordinarily be hidden in the darkness. If the face is in-line with
            the light, e.g. cliffs at noon, and there is significant normal
            bending, then the polygon may be almost fully lit, even though the
            polygon is nearly at 90 degrees to the sun.

            There used to be a material property <def>shadowObliqueCutoff</def>
            for controlling this effect, but it is no-longer implemented since
            the switch to deferred shading.  The technique was to attenuate
            shadows more aggressively on surfaces rendered with the material in
            question.  However doing this on a per-material basis causes areas
            of the mesh that do not have normal bending to be subject to the
            same attenuation of shadows.  The preferred solution is to
            calculate the amount the amount required at each vertex and store
            that in the vertex as an attribute.  This can be fully automated in
            the asset generation pipeline.  However it is not yet implemented.
            Please ignore the artifacts for now.

        </section>

        <section title="Shadow Acne" id="gfx_shadow_acne">

            <image src="shadow_acne_diagram.jpg" title="Shadow Acne Diagram">
            An illustration of shadow acne.
            </image>

            Imprecision in the shadow map, which records the distance of each
            occluder from the light, causes the shadow to fluctuate, causing
            unpleasant high frequency transitions from 'in shadow' to 'not in
            shadow' on every surface that faces the light. The engine will
            avoid shadow acne by adding a certain amount of bias to the depth
            of hte caster during the shadow casting phase.  Thus, the shadow is
            pushed away from the light by enough in order to avoid the noise
            being an issue.  The following image illustrates the problem and
            how the depth bias solves it, the screen shot is of a natural gas
            tank, but the diagram below is for a wall on flat ground.

            The engine tries to use the minimal amount of bias to avoid shadow
            acne, by using the normal of the casting surface and a small
            constant offset on everything.  Thus, you don't need to worry about
            this as a modeller, unless your surfaces are so thin that even this
            small amount of bias is too much.

        </section>

        <section title="Additional Bias" id="gfx_shadow_bias">

            <image src="shadow_unwanted.jpg" title="Unwanted Shadow Fidelity">
            Unwanted self-shadowing.
            </image>

            You can add additional bias yourself, in the material, in order to
            get rid of other artefacts. For example here there are unwanted
            shadows on the tank. There is simply not enough fidelity in the
            dynamic shadows to properly render shadows for such detailed
            geometry.  We would rather there were no shadows at all.

            One way to avoid this is to avoid these kind of nooks and crannies
            in the geometry of the object. However since these contribute
            greatly to the appearance of objects, this may be unacceptable.
            Another solution is to add another 0.1m to the depth bias during
            shadow casting (on top of the small amount calculated by the
            engine), in order to push the shadow far enough away from the
            object to hide shadows on the high detail parts of the mesh.

        </section>

        <section title="Shadow Disconnection" id="gfx_shadow_discon">

            <image src="shadow_disconnection.jpg" title="Shadow Disconnection">
                Too much bias causes the shadow to disconnect from the base of
                the object.
            </image>

            Too much bias can cause a problem in itself though. If the bias is
            increased enough, the shadow will move so far from the object that
            there will be a 'gap' where the object meets the ground. This gives
            the unwelcome appearance that the object is 'floating' above the
            ground, as seen with this table.  If you want a lot of bias, you
            may have to thicken the geometry of your model.

            The bias automatically used by the engine is carefully chosen to be
            as small as it can be. However as a modeller you must also make
            sure your additional bias is not too large as well.

        </section>

    </section>

    <section title="Sky Bodies" id="gfx_sky_body">

        The sky is rendered in a special way.  The scene is rendered first,
        including lighting (but not post-processing effects like bloom), and
        then any areas that remain undrawn are filled with the sky.  The sky
        has only emissive lighting, as it is too far away to be affected by
        lights in the scene.  It also does not move, it is always centered at
        the camera.

        The sky is composed of a number of layers, composed via alpha blending
        in HDR buffers.  Conceptually, each layer is a <def>Sky Body</def>
        whose polygons either completely or partially surround the camera
        (which is at 0,0,0).  Thus, the body does not have a position like the
        regular <sref id="gfx_body">graphics body</sref>, although it can
        be rotated.

        Different layers allow the controlling of different effects, e.g. you
        can have a base stars layer, then a moon layer (which you rotate to
        control the position of the moon), followed by a sky layer (with some
        sort of atmospheric colour model), finally followed by clouds.  Each of
        these layers can be enabled/disabled and oriented seprately.

        Each sky body uses a mesh that is exported from a modeller in the usual
        way.  It is typically a cube or sphere, or in the case of sun / moon
        etc, a small billboard.  One can also imagine strips or other shapes
        being used in certain cases, like meteorite belts or jet trails.  The
        polygons all point inwards, towards vec(0,0,0).  The distance of the
        polygons from 0,0,0 can be arbitrary, since the depth buffer is not
        used to control the layering of sky bodies, rather they are layered
        explicitly.  Typically we use about 1 metre because that is easy to
        manipulate in modelling software.

        Sky body materials refer to sky materials instead of <sref
        id="gfx_materials_and_shaders">regular materials</sref>.  Sky materials are defined
        in a special way, and are not interchangeable with regular materials,
        although they can share textures.  Here is an example:

        <lua>
            sky_material `starfield` {
                emissiveMap = `starfield.dds`;
            }
        </lua>

        There is currently implemented a prototype for custom sky shaders, that
        interact with the sky materials.  This is going to be changed in future
        to make it easier to use, so we shall not document it here.  Take a
        look at /system/sky.lua if you are curious to see the current state.
        If you need advanced skies now, contact us via IRC or the forums for
        assistance.

        The sky body itself is created as shown below:

        <lua>
            sky_ent = gfx_sky_body_make(`MySkyMesh.mesh`, 255)
            sky_ent.enabled = true/false -- disable drawing of this layer
            sky_ent.zOrder = 100 -- change z order
            sky_ent.orientation = quat(1,0,0,0)
        </lua>

        The 255 is the z order, an integer between 0 and 255 inclusive.  The
        value of 255 places it behind everything else.  Use lower numbers to
        add more layers on top.  If two sky bodies have the same z order, the
        ordering is undefined.  So don't do that unless you know the polys will
        not overlap.

    </section>

    <section title="Lights" id="gfx_light">

        Lights provide illumination of the scene (beyond that of the sun/moon)
        and also provide a corona sprite.  They exist at a given place in the
        scene and (if set up to be spotlights) can be rotated to light in a
        given direction.  They are implemented using deferred shading, which
        means their performance cost is proportional to the number of screen
        pixels lit by them.  This allows lots of small lights to be rendered
        efficiently.

        The lights are nodes in the scene graph, so can be attached to other
        nodes and have the same fields.  Like graphics bodies, they can be
        individually enabled/disabled and faded.

        <image src="light_diagram.jpg" title="Light Diagram">
        Illustration of various light fields.
        </image>

        <lua>
            l = gfx_light_make()
            l.localPosition = vec(1, 2, 3)
            l.orientation = quat(1, 0, 0, 0)
            l.diffuseColour = vec(2,2,2) -- note use of HDR
            l.specularColour = vec(2,2,2)
            l.range = 10 -- light extends for 10m
            l.innerAngle = 50  -- see diagram
            l.outerAngle = 60
        </lua>

        The outerAngle must be >= the innerAngle, as shown in the diagram.  If
        the innerAngle is 180, then the light is not directional, so the
        orientation does not matter.  Otherwise it shines in the +Y direction
        unless oriented differently.

        <section title="Coronas" id="coronas">

            If desired, Grit can draw a corona to simulate the blinding effect
            of the light itself.  The corona's size (diameter) must be
            specified, as it defaults to 0 which means no corona.  The corona
            also has its own colour, which you must set otherwise it defaults
            to white.

            <lua>
                l.coronaSize = 0.3 -- sphere diameter in metres
                l.coronaLocalPosition = vec(0, 0, 0)
                l.coronaColour = vec(1,1,1) -- white
            </lua>

        </section>

    </section>

    <section title="Particles" id="gfx_particle">

        Particles are a special kind of graphical effect.  Conceptually they
        are cubes in the game world, i.e., they have a 3D position and size,
        but their orientation is constrained to face the direction of the
        camera (and optionally to be rotated around the vector to the camera).
        Although they are conceptually 3D, they are rendered with textured 2D
        quads that face the camera.  The shader uses the screen depth to soften
        their intersection with other geometry in the scene.

        Particles are typically used to render certain types of volumetric
        gas-like effects like smoke, clouds, kicked up dirt, flames, and
        explosions.  Each particle is a fixed, roughly spherical shape, but
        with a cloud of particles, the overall shape can be roughly
        approximated.

        Although the term "particles" implies that there should be a large
        number of small particles, typically this is not the case.  Particles
        are reasonably heavy to render and you don't want to have too many
        behind each other in the case of alpha particles because that causes a
        lot of overdraw.

        Use just enough particles to represent your 3D shape.  The key to
        particles is to use a great looking (preferably animated) texture, and
        then write enough behaviour into that particle class, that a cloud of
        particles together resemble what you want.

        There are a number of particles defined in /common, which you can spawn
        in your own code.  Typically a particle is created at a given location,
        possibly with customised values for some other parameters , and then
        from that point on it is in charge of its own destiny, including
        destroying itself.  Particle behaviour can be arbitrarily complex,
        performing physics tests or interacting with objects in the scene.
        They can even spawn more particles.

        <todo>The particles API needs some cleanup before it is worth documenting.</todo>

        <lua>
        </lua>

        To create your own particles, define a <def>Particle Class</def> in Lua
        scripting, which provides the references textures, default fields, and provides a
        stepCallback to implement the behaviour of the particle.

        Particle texture coordinates are used to implement animation.  When
        defining a particle, a list of coordinates are given, one for each
        frame.

        Particles have a number of other attributes that define their
        appearance: colour, alpha, position, angle, and size.

    </section>

    <include src="hud.xml"/>

    <section title="Fonts" id="gfx_font">

        The font subsystem allows the definition of fonts that can then be used
        to render text in the <sref id="gfx_hud">HUD</sref> and, in future, in
        the game world (e.g., for signs, etc).  Each font has a name in the
        directory hierarchy, so they should be referred to using the `` quotes.
        There are many examples in /common/fonts.

        The font texture contains all the letters for a particular size.  It
        commonly has an alpha channel to allow the text to be rendered on a
        background, and is typically white, so the text colour can be
        controlled with a mask at render time.  However you can use colours if
        you wish.  This can be useful in some cases, for big title text and so
        on.

        A font binds from each unicode code point to a rectangle within the
        texture.  Not every code point need be bound.  If a code point is not
        bound in a font, then the font subsystem tries to find another
        character that is.  First it tries the unicode replacement character
        "?", then space " " then finally the letter "E", before giving up and
        skipping the character.

        To define a font, use the <def>gfx_font_define</def> function.  This
        takes the name of the font being defined, the name of the texture to
        use, the line spacing (font height), and then a table giving the
        texture co-ordinates for each codepoint.  Each code point defines a
        rectangle of texels, by giving the x, y of the bottom left corner of
        the rectangle, and also the width and height.  The texture co-ordinates
        are relative to the bottom left of the texture.  The following font
        defines codepoints for the letters A and B, and maps the same
        rectangles to a and b as well.  This can be a useful way of making text
        appear in all capitals.

        <lua>
            gfx_font_define(`MyFont`, `MyFont.jpg`, 12, {
                [0x0041] = {0, 0, 10, 12},  -- A
                [0x0042] = {0, 10, 10, 12},  -- B
                [0x0061] = {0, 0, 10, 12},  -- a
                [0x0062] = {0, 10, 10, 12},  -- b
            })
        </lua>

        In the above case, both letters are the same size, but this need not be
        the case.  One can define variable width fonts, which are often more
        attractive than monospaced fonts.  If the letters are different
        heights, they are aligned along the top of the rectangles as the text
        is rendered.  This means you can clip unused texels at the bottom of
        character, saving texture space.

    </section>

</section>

